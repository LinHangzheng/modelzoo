# GPT-3 13B model

### Input
train_input:
    data_processor: "GptHDF5DataProcessor"
    data_dir:
       - ./language/datasets/pile_original/hdf5_dataset/train_shuffled_msl2048/0/
       - ./language/datasets/pile_original/hdf5_dataset/train_shuffled_msl2048/1/
       - ./language/datasets/pile_original/hdf5_dataset/train_shuffled_msl2048/2/
       - ./language/datasets/pile_original/hdf5_dataset/train_shuffled_msl2048/3/
       - ./language/datasets/pile_original/hdf5_dataset/train_shuffled_msl2048/4/
       - ./language/datasets/pile_original/hdf5_dataset/train_shuffled_msl2048/5/
       - ./language/datasets/pile_original/hdf5_dataset/train_shuffled_msl2048/6/
       - ./language/datasets/pile_original/hdf5_dataset/train_shuffled_msl2048/7/
       - ./language/datasets/pile_original/hdf5_dataset/train_shuffled_msl2048/8/
       - ./language/datasets/pile_original/hdf5_dataset/train_shuffled_msl2048/9/
    vocab_size: 50257
    max_sequence_length: 2048
    shuffle: True
    shuffle_seed: 256
    repeat: True
    # GPT-3 13B trained with total batch size 1024 seqs, MSL 2048
    # For high utilization in weight streaming, we estimate batch size >24 seqs/CS-1
    # Critical batch size is expected to be roughly 250 seqs at full OpenAI
    # convergence, so we should aim for batch size maybe 24 * 8 grad accums = 192
    batch_size: 60
    use_multiple_workers: False  # Set to True when using multiple workers on CS-1.
    mixed_precision: True

eval_input:
    data_processor: "GptHDF5DataProcessor"
    data_dir: "./language/datasets/pile_original/hdf5_dataset/val_msl2048/"
    vocab_size: 50257
    max_sequence_length: 2048
    shuffle: False
    batch_size: 60
    mixed_precision: True

### Model
model:
    # Embedding
    vocab_size: 50257
    hidden_size: 5120 # NOTE: Paper says "5140" but was typo. Confirmed with authors
    use_position_embedding: True
    position_embedding_type: "learned"
    share_embedding_weights: True
    max_position_embeddings: 2048

    # Encoder
    num_hidden_layers: 40
    dropout_rate: 0.0
    layer_norm_epsilon: 1.0e-5 # change to 1.0e-12 for single precision training

    # Encoder - Attention
    attention_softmax_fp32: False
    num_heads: 40
    attention_type: "scaled_dot_product"
    attention_dropout_rate: 0.0
    use_projection_bias_in_attention: True
    use_ffn_bias_in_attention: True

    # Encoder - ffn
    filter_size: 20480
    nonlinearity: "gelu"
    use_ffn_bias: True

    embedding_initializer:
       name: "truncated_normal"
       mean: 0.0
       std: 0.02

    initializer:
       name: "truncated_normal"
       mean: 0.0
       std: 0.02

    output_layer_initializer:
      name: "truncated_normal"
      mean: 0.0
      std: 0.0022360679774997894 # 0.02 / sqrt(2 * num_hidden_layers)

    # Task-specific
    use_bias_in_output: False
    loss_scaling: "batch_size"
    loss_weight: 0.00048828125 # = 1/max_sequence_length

    # Cerebras parameters
    mixed_precision: True
    precision_opt_level: 1
    boundary_casting: False
    tf_summary: False

### Optimization
optimizer:
    optimizer_type: "adamw"
    beta1: 0.9
    beta2: 0.95
    eps: 1.0e-8
    correct_bias: True
    weight_decay_rate: 0.01
    max_gradient_norm: 1.0
    # GPT-3 warms-up training over the first 375M tokens
    # Then, the DeepMind approach cosine decays learning rate over the rest
    # of the training steps
    learning_rate:
     - scheduler: "Linear"
       initial_learning_rate: 0.0
       end_learning_rate: 4.5e-5
       steps: 3000
     - scheduler: "CosineDecay"
       initial_learning_rate: 4.5e-5
       end_learning_rate: 4.5e-6
       decay_steps: 262000
       steps: 262000
    loss_scaling_factor: "dynamic"
    initial_loss_scale: 2147483648.0
    max_loss_scale: 2147483648.0
    log_summaries: True
    ws_summary: True

### Cerebras parameters
runconfig:
  max_steps: 162500
  eval_steps: 1528
  save_summary_steps: 1
  checkpoint_steps: 0
  keep_checkpoint_max: 21
  log_step_count_steps: 1
  log_steps: 1
  save_losses: True
  save_initial_checkpoint: True
  seed: 0
  enable_distributed: False # Change to True on GPU
  num_wgt_servers: 24
