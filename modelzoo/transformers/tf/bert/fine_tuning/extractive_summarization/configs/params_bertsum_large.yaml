# BertSummarizationModel config file. Large Bert backbone is used.


### Input
train_input:
    data_dir: "./language/datasets/extractive_summarization/cnn_dailymail/train/"
    data_processor: BertSumTfRecordsProcessor
    vocab_file: "../../../../vocab/google_research_uncased_L-12_H-768_A-12.txt"
    max_sequence_length: 512
    max_cls_tokens: 50
    batch_size: 3

eval_input:
    data_dir: "./language/datasets/extractive_summarization/cnn_dailymail/eval/"
    data_processor: BertSumTfRecordsProcessor
    vocab_file: "../../../../vocab/google_research_uncased_L-12_H-768_A-12.txt"
    max_sequence_length: 512
    max_cls_tokens: 50
    batch_size: 3

model:
    pretrain_params_path: "../../configs/params_bert_large_msl512.yaml"
    mixed_precision: True
    boundary_casting: False
    tf_summary: False
    # average number of CLS tokens per batch = 16, thus 1.0 / 16 = 0.0625.
    loss_weight: 0.0625 # `loss_weight` allows us to apply scaling of the loss
                     # not just across the batch size, but also
                     # across number of CLS tokens with this formula
                     # `loss_weight` = 1 / {average number of CLS tokens per batch}
                     # and the `loss_weight_type` set to "batch_size".
    enable_gpu_optimizations: False

### Optimization
optimizer:
    # CS1 will fail if grad_accum_steps is set, since this is a gpu-only setting.
    # grad_accum_steps: 2
    optimizer_type: "adam" # {"sgd", "momentum", "adam", "adamw"}
    beta_1: 0.9
    beta_2: 0.999
    learning_rate:
        - steps: 10000
          scheduler: "Linear"
          initial_learning_rate: 0.0
          end_learning_rate: 0.0001
        - scheduler: "Linear"
          initial_learning_rate: 0.0001
          end_learning_rate: 2.2361e-4
          steps: 50000
    loss_scaling_factor: "dynamic"
    log_summaries: True

### Cerebras parameters
runconfig:
    max_steps: 50000
    save_summary_steps: 1000
    save_checkpoints_steps: 1000
    eval_steps: 0 # eval_steps <= 0 means we use all of the eval data
    keep_checkpoint_max: 3
    throttle_secs: 0
