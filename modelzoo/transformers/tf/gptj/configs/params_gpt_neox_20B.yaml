################################################
## Base configuration: GPT-NeoX 20B params
## Reference for Gpt-NeoX :: https://github.com/EleutherAI/gpt-neox
################################################


train_input:
  data_processor: "GptTfRecordsProcessor"
  data_dir:
    - "./language/datasets/pile_original/new_tfrecords/gpt2_encoding/train_msl2048/0/"
    - "./language/datasets/pile_original/new_tfrecords/gpt2_encoding/train_msl2048/1/"
    - "./language/datasets/pile_original/new_tfrecords/gpt2_encoding/train_msl2048/2/"
    - "./language/datasets/pile_original/new_tfrecords/gpt2_encoding/train_msl2048/3/"
    - "./language/datasets/pile_original/new_tfrecords/gpt2_encoding/train_msl2048/4/"
    - "./language/datasets/pile_original/new_tfrecords/gpt2_encoding/train_msl2048/5/"
    - "./language/datasets/pile_original/new_tfrecords/gpt2_encoding/train_msl2048/6/"
    - "./language/datasets/pile_original/new_tfrecords/gpt2_encoding/train_msl2048/7/"
    - "./language/datasets/pile_original/new_tfrecords/gpt2_encoding/train_msl2048/8/"
    - "./language/datasets/pile_original/new_tfrecords/gpt2_encoding/train_msl2048/9/"
  vocab_size: 50432
  max_sequence_length: 2048
  shuffle: True
  repeat: True
  batch_size: 50

eval_input:
  data_processor: "GptTfRecordsProcessor"
  data_dir: "./language/datasets/pile_original/new_tfrecords/gpt2_encoding/val_msl2048/"
  vocab_size: 50432
  max_sequence_length: 2048
  shuffle: False
  repeat: False
  batch_size: 50

model:
  share_embedding_weights: False
  max_position_embeddings: 2048

  hidden_size: 6144
  num_heads: 64
  num_hidden_layers: 44

  use_untied_layer_norm: True
  use_projection_bias_in_attention: True
  use_ffn_bias_in_attention: True
  use_ffn_bias: True

  filter_size: 24576
  nonlinearity: "gelu"

  rotary_dim: 24
  layer_norm_epsilon: 1.0e-5
  use_bias_in_output: False

  embedding_initializer:
    - name: "variance_scaling"
      scale_type: "small_init"
      mode: "fan_out"
      distribution: "untruncated_normal"

  initializer:
    - name: "variance_scaling"
      scale_type: "small_init"
      distribution: "untruncated_normal"

  output_layer_initializer:
    - name: "variance_scaling"
      scale_type: "wang_init"
      distribution: "untruncated_normal"

  mixed_precision: True
  precision_opt_level: 0
  boundary_casting: False
  tf_summary: False

optimizer:
  optimizer_type: "adamw"
  beta1: 0.9
  beta2: 0.95
  epsilon: 1.0e-8
  weight_decay_rate: 0.01
  max_gradient_norm: 1.0
  use_bias_correction: True
  max_loss_scale: 4290774016.0
  learning_rate:
    - scheduler: "Linear"
      initial_learning_rate: 0.0
      end_learning_rate: 0.97e-4
      steps: 1500
    - scheduler: "Cosine"
      initial_learning_rate: 0.97e-4
      alpha: 0.1
      decay_steps: 148500
  loss_scaling_factor: "dynamic"

runconfig:
  max_steps: 150000
  save_summary_steps: 500
  log_step_count_steps: 10
  save_checkpoints_steps: 5000
  keep_checkpoint_max: 0
  enable_distributed: False
  num_wgt_servers: 24
