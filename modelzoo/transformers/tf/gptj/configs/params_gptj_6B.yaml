################################################
## Base configuration: GPT-J 6B params
## Reference for GPT-J :: https://github.com/kingoflolz/mesh-transformer-jax
## Note: this is adapted from the jax version referenced above for CS-2
## compatibility. In particular, the batch size is reduced and learning rate
## and steps are adjusted accordingly. The original jax version has
## batch_size: 512
## peak learning rate: 1.2e-4
## warmup steps: 3k
## total steps: 350k
################################################


train_input:
  data_processor: "GptTfRecordsProcessor"
  data_dir:
    - "./language/datasets/pile_original/new_tfrecords/gpt2_encoding/train_msl2048/0"
    - "./language/datasets/pile_original/new_tfrecords/gpt2_encoding/train_msl2048/1"
    - "./language/datasets/pile_original/new_tfrecords/gpt2_encoding/train_msl2048/2"
    - "./language/datasets/pile_original/new_tfrecords/gpt2_encoding/train_msl2048/3"
    - "./language/datasets/pile_original/new_tfrecords/gpt2_encoding/train_msl2048/4"
    - "./language/datasets/pile_original/new_tfrecords/gpt2_encoding/train_msl2048/5"
    - "./language/datasets/pile_original/new_tfrecords/gpt2_encoding/train_msl2048/6"
    - "./language/datasets/pile_original/new_tfrecords/gpt2_encoding/train_msl2048/7"
    - "./language/datasets/pile_original/new_tfrecords/gpt2_encoding/train_msl2048/8"
    - "./language/datasets/pile_original/new_tfrecords/gpt2_encoding/train_msl2048/9"
  vocab_size: 50400
  max_sequence_length: 2048
  shuffle: True
  repeat: True
  batch_size: 65 # Ideally we want batch size 512, but this doesn't fit on system yet

eval_input:
  data_processor: "GptTfRecordsProcessor"
  data_dir: "./language/datasets/pile_original/new_tfrecords/gpt2_encoding/val_msl2048"
  vocab_size: 50400
  max_sequence_length: 2048
  shuffle: False
  repeat: False
  batch_size: 65 # Ideally we want batch size 512, but this doesn't fit on system yet

model:
  # Embedding
  hidden_size: 4096
  share_embedding_weights: False
  max_position_embeddings: 2048

  # Encoder
  num_hidden_layers: 28
  rotary_dim: 64
  layer_norm_epsilon: 1.0e-5 # change to 1.0e-12 for single precision training

  # Encoder - Attention
  num_heads: 16
  use_projection_bias_in_attention: False
  use_ffn_bias_in_attention: False

  # Encoder - ffn
  filter_size: 16384
  nonlinearity: "gelu"
  use_ffn_bias: True

  # Task-specific
  use_bias_in_output: True

  embedding_initializer:
    - name: "scaled_init_normal"
      key: "vocab_size"

  initializer:
    - name: "variance_scaling"
      scale: 1.0

  output_layer_initializer:
    - name: "variance_scaling"
      scale_type: "wang_init"

  mixed_precision: True
  boundary_casting: False
  tf_summary: False
  precision_opt_level: 0

optimizer:
  optimizer_type: "adamw"
  epsilon: 1.0e-8
  weight_decay_rate: 0.1
  max_gradient_norm: 1.0
  use_bias_correction: True
  max_loss_scale: 4290774016.0
  learning_rate:
    - scheduler: "Linear"
      initial_learning_rate: 0.0
      end_learning_rate: 4.3e-5
      steps: 23630 # approx 1.5M samples
    - scheduler: "Cosine"
      initial_learning_rate: 4.3e-5
      alpha: 0.1
      decay_steps: 2363000 # approx 150M samples
    - scheduler: "Constant"
      learning_rate: 4.3e-6
  loss_scaling_factor: "dynamic"
  log_summaries: True

runconfig:
  max_steps: 2700000 # approx 175M samples
  save_summary_steps: 500
  save_checkpoints_steps: 10000
  keep_checkpoint_max: 0
  enable_distributed: False
