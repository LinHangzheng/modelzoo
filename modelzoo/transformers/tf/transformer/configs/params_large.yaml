# Copyright 2022 Cerebras Systems.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Transformer Large - according to Table 3.
# Based on transformer_big(), but using transformer_base_v1() instead of transformer_base()
# https://github.com/tensorflow/tensor2tensor/blob/5623deb79cfcd28f8f8c5463b58b5bd76a81fd0d/tensor2tensor/models/transformer.py#L1981
### Input.
train_input:
    data_processor: "TransformerDynamicDataProcessor"
    src_data_dir: "./transformer/wmt16_en_de/train.tok.clean.bpe.32000.en"
    tgt_data_dir: "./transformer/wmt16_en_de/train.tok.clean.bpe.32000.de"
    src_vocab_file: "./transformer/wmt16_en_de/vocab.bpe.32000.en"
    tgt_vocab_file: "./transformer/wmt16_en_de/vocab.bpe.32000.de"
    src_max_sequence_length: 256
    tgt_max_sequence_length: 256
    batch_size: 2048
    buckets: 1
    repeat: True
    shuffle: True
    shuffle_buffer: 50000
    shuffle_seed: 12345
    use_multiple_workers: True # Set to False when running on GPU.

eval_input:
    data_processor: "TransformerDynamicDataProcessor"
    src_data_dir: "./transformer/wmt16_en_de/newstest2014.tok.clean.bpe.32000.en"
    tgt_data_dir: "./transformer/wmt16_en_de/newstest2014.tok.clean.bpe.32000.de"
    src_vocab_file: "./transformer/wmt16_en_de/vocab.bpe.32000.en"
    tgt_vocab_file: "./transformer/wmt16_en_de/vocab.bpe.32000.de"
    src_max_sequence_length: 256
    tgt_max_sequence_length: 256
    batch_size: 16
    buckets: 1

model:
    # Embedding.
    share_encoder_decoder_embedding: True
    position_embedding_type: "fixed"  # valid options: "learned", "fixed"
    hidden_size: 1024 # d_model

    # Encoder/Decoder.
    encoder_num_hidden_layers: 6
    decoder_num_hidden_layers: 6
    dropout_rate: 0.1
    layer_norm_epsilon: 1.0e-5

    # Encoder/Decoder Attention.
    num_heads: 16
    attention_type: "scaled_dot_product"  # valid options: "dot_product", "scaled_dot_product"
    attention_dropout_rate: 0.1
    use_projection_bias_in_attention: False
    use_ffn_bias_in_attention: False

    # Encoder/Decoder - ffn.
    filter_size: 4096  # d_ff
    encoder_nonlinearity: "relu"
    decoder_nonlinearity: "relu"
    use_ffn_bias: True

    # Cerebras configs.
    dropout_seed: 0
    weight_initialization_seed: 0
    mixed_precision: True
    boundary_casting: False
    tf_summary: False
    use_vsl: True


### Optimization.
optimizer:
    optimizer_type: "adam"
    beta1: 0.9
    beta2: 0.98
    epsilon: 1.0e-6
    disable_lr_steps_reset: False
    learning_rate:
      - end_learning_rate: 0.000698683912937353
        initial_learning_rate: 1.0e-07
        scheduler: Linear
        steps: 4000
      - end_learning_rate: 0.00037894798246424215
        initial_learning_rate: 0.000698683912937353
        scheduler: Linear
        steps: 9600
      - end_learning_rate: 0.00029014271289331547
        initial_learning_rate: 0.00037894798246424215
        scheduler: Linear
        steps: 9600
      - end_learning_rate: 0.00024401778319601815
        initial_learning_rate: 0.00029014271289331547
        scheduler: Linear
        steps: 9600
      - end_learning_rate: 0.00021462335024901533
        initial_learning_rate: 0.00024401778319601815
        scheduler: Linear
        steps: 9600
      - end_learning_rate: 0.00019380240931989222
        initial_learning_rate: 0.00021462335024901533
        scheduler: Linear
        steps: 9600
      - end_learning_rate: 0.00017806195541597027
        initial_learning_rate: 0.00019380240931989222
        scheduler: Linear
        steps: 9600
      - end_learning_rate: 0.00016562350566866137
        initial_learning_rate: 0.00017806195541597027
        scheduler: Linear
        steps: 9600
      - end_learning_rate: 0.00015547359888418554
        initial_learning_rate: 0.00016562350566866137
        scheduler: Linear
        steps: 9600
      - end_learning_rate: 0.00014698682270697125
        initial_learning_rate: 0.00015547359888418554
        scheduler: Linear
        steps: 9600
      - end_learning_rate: 0.00013975354982773464
        initial_learning_rate: 0.00014698682270697125
        scheduler: Linear
        steps: 9600
    loss_scaling_factor: "dynamic"
    log_summaries: True

### Cerebras parameters.
runconfig:
    max_steps: 300000
    eval_steps: 187
    save_summary_steps: 100
    save_checkpoints_steps: 10000
    keep_checkpoint_max: 5
    tf_random_seed: 1202
    enable_distributed: False

### CS-specific configurations.
csconfig:
    use_cbfloat16: False
