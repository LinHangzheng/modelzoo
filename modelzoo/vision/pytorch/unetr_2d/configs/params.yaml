# Params for FC MNIST model
description: "UNet base model params"

train_input: 
    shuffle: True
    shuffle_buffer_size : 10000
    augment_data: True
    data_dir: 'data' # Place to store data
    shuffle: True
    shuffle_seed: 1
    num_parallel_calls: 0   # 0 means AUTOTUNE
    num_classes: 11
    # IR_threshould: 0.18
    image_shape: [224,224] #128
    dataset: "IR_dataset"
    IR_channel_level: 17 # up to 19
    noise_variance: 0
    augment_data: True
    batch_size: 2
    num_workers: 0
    drop_last: False
    prefetch_factor: 3
    class_id: 5

eval_input: &eval_input
    <<: *train_input
    augment_data: False
    shuffle: False
    batch_size: 1
    num_workers: 2

model:
    embed_dim: 1024
    patch_size: 16
    num_heads: 16
    dropout: 0.1
    mlp_hidden: 4096
    num_layers: 24
    ext_layers: [6,12,18,24] #[3, 6, 9, 12]

optimizer:
    lr: 0.0001
    optimizer_type: 'adamw' # {'sgd', 'momentum', 'adam', 'adamw'}
    weight_decay_rate: 5e-5
    epsilon: 1e-6
    max_gradient_norm: 1.0
    disable_lr_steps_reset: True
    learning_rate:
        - steps: 10000
          scheduler: 'Linear'
          initial_learning_rate: 0.0
          end_learning_rate: 0.0001
        - scheduler: 'Linear'
          initial_learning_rate: 0.0001
          end_learning_rate: 0.0
          steps: 1000000
    loss_scaling_factor: 'dynamic'


runconfig:
    max_steps: 8000
    log_steps: 100
    checkpoint_steps: 500
    seed: 1
    show_debug_metrics: False
    save_losses: True
    save_initial_checkpoint: True
    num_wgt_servers: 1
